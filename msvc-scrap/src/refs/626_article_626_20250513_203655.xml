<record>
  <isbns></isbns>
  <shortDBName>edssch</shortDBName>
  <isiType>GEN</isiType>
  <notes></notes>
  <mid></mid>
  <language></language>
  <source>npj Computational Materials</source>
  <bookEdition></bookEdition>
  <title>Systematic softening in universal machine learning interatomic potentials</title>
  <pageEnd></pageEnd>
  <pageStart></pageStart>
  <peerReviewed></peerReviewed>
  <isOpenAccess></isOpenAccess>
  <publicationDate>20250101</publicationDate>
  <pageCount></pageCount>
  <publisherLocations></publisherLocations>
  <issue>1</issue>
  <identifiers></identifiers>
  <subjects>Chemical Sciences ; Physical Chemistry ; Networking and Information Technology R&amp;amp;D (NITRD) ; Machine Learning and Artificial Intelligence ; Theoretical and computational chemistry ; Materials engineering ; Condensed matter physics</subjects>
  <abstract>Machine learning interatomic potentials (MLIPs) have introduced a new paradigm for atomic simulations. Recent advancements have led to universal MLIPs (uMLIPs) that are pre-trained on diverse datasets, providing opportunities for universal force fields and foundational machine learning models. However, their performance in extrapolating to out-of-distribution complex atomic environments remains unclear. In this study, we highlight a consistent potential energy surface (PES) softening effect in three uMLIPs: M3GNet, CHGNet, and MACE-MP-0, which is characterized by energy and force underprediction in atomic-modeling benchmarks including surfaces, defects, solid-solution energetics, ion migration barriers, phonon vibration modes, and general high-energy states. The PES softening behavior originates primarily from the systematically underpredicted PES curvature, which derives from the biased sampling of near-equilibrium atomic arrangements in uMLIP pre-training datasets. Our findings suggest that a considerable fraction of uMLIP errors are highly systematic, and can therefore be efficiently corrected. We argue for the importance of a comprehensive materials dataset with improved PES sampling for next-generation foundational MLIPs.</abstract>
  <pubTypes>Academic Journal</pubTypes>
  <an>edssch.oai:escholarship.org:ark:/13030/qt7zz1s3tj</an>
  <docTypes>article</docTypes>
  <volume>11</volume>
  <issns></issns>
  <degreeLevel></degreeLevel>
  <plink>https://research.ebsco.com/linkprocessor/plink?id=613d5a0e-3d81-3622-a9ec-f9b0eff49c9c</plink>
  <doids></doids>
  <publisher>eScholarship, University of California</publisher>
  <contributors>Deng, Bowen ; Choi, Yunyeong ; Zhong, Peichen ; Riebesell, Janosh ; Anand, Shashwat ; Li, Zhuohan ; Jun, KyuJung ; Persson, Kristin A ; Ceder, Gerbrand</contributors>
  <coverDate>2025</coverDate>
  <longDBName>eScholarship</longDBName>
  <doi></doi>
</record>
