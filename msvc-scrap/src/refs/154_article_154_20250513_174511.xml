<record>
  <isbns>979-8-3315-1579-9 ; 979-8-3315-1578-2</isbns>
  <shortDBName>edseee</shortDBName>
  <isiType>GEN</isiType>
  <notes></notes>
  <mid></mid>
  <language></language>
  <source>2025 24th International Symposium INFOTEH-JAHORINA (INFOTEH), INFOTEH-JAHORINA (INFOTEH), 2025 24th International Symposium</source>
  <bookEdition></bookEdition>
  <title>CPU vs. GPU: Performance Evaluation of Classical Machine and Deep Learning Algorithms</title>
  <pageEnd>6</pageEnd>
  <pageStart>1</pageStart>
  <peerReviewed>true</peerReviewed>
  <isOpenAccess></isOpenAccess>
  <publicationDate>20250319</publicationDate>
  <pageCount>6</pageCount>
  <publisherLocations></publisherLocations>
  <issue></issue>
  <identifiers></identifiers>
  <subjects>Bioengineering ; Communication, Networking and Broadcast Technologies ; Components, Circuits, Devices and Systems ; Computing and Processing ; Power, Energy and Industry Applications ; Robotics and Control Systems ; Signal Processing and Analysis ; Deep learning ; Training ; Support vector machines ; Performance evaluation ; Computational modeling ; Linear regression ; Graphics processing units ; Inference algorithms ; Hardware ; Central Processing Unit ; deep learning ; machine learning ; performance analysis ; CPU ; GPU ; hardware benchmarking</subjects>
  <abstract>The swift progress of various types of machine learning and deep learning models necessitated the development of computational performance benchmarks. This study provides a performance evaluation analysis of the classical machine and deep learning algorithms executed on two different hardware architectures: the central processing units (CPUs) and the graphics processing units (GPUs). The experiments include various models such as linear regression, support vector machines, random forests, convolutional neural networks, and others. In this comparative analysis, the training and inference time are assessed. Additionally, trade-offs between CPU and GPU execution are presented. The results provide a set of recommendations for informed selection of optimal hardware for different machine learning models. It can serve as a guide for practitioners with a goal of enhancing the efficiency of their classical machine and deep learning models.</abstract>
  <pubTypes>Conference Paper</pubTypes>
  <an>edseee.10959248</an>
  <docTypes>Conference</docTypes>
  <volume></volume>
  <issns>27679470</issns>
  <degreeLevel></degreeLevel>
  <plink>https://research.ebsco.com/linkprocessor/plink?id=a8ce5913-3b58-3a7a-9132-704de54a8da1</plink>
  <doids></doids>
  <publisher>IEEE</publisher>
  <contributors>Staka, Zorana ; Misic, Marko ; Tomasevic, Milo</contributors>
  <coverDate>20250319</coverDate>
  <longDBName>IEEE Xplore Digital Library</longDBName>
  <doi>10.1109/INFOTEH64129.2025.10959248</doi>
</record>
